{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jalam1001/huggingface/blob/main/Copy_of_LFM2_MoE_Inference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "iqLRc-GH_QTh"
      },
      "outputs": [],
      "source": [
        "!pip install -qqq git+https://github.com/huggingface/transformers.git@0c9a72e4576fe4c84077f066e585129c97bfd4e6 --progress-bar off\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Load model and tokenizer\n",
        "model_id = \"LiquidAI/LFM2-8B-A1B\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"auto\",\n",
        "    dtype=\"bfloat16\",\n",
        "#    attn_implementation=\"flash_attention_2\" <- uncomment on compatible GPU\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "\n",
        "# Generate answer\n",
        "prompt = \"What is C. elegans?\"\n",
        "input_ids = tokenizer.apply_chat_template(\n",
        "    [{\"role\": \"user\", \"content\": prompt}],\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        "    tokenize=True,\n",
        ").to(model.device)\n",
        "\n",
        "output = model.generate(\n",
        "    input_ids,\n",
        "    do_sample=True,\n",
        "    temperature=0.3,\n",
        "    min_p=0.15,\n",
        "    repetition_penalty=1.05,\n",
        "    max_new_tokens=512,\n",
        ")\n",
        "\n",
        "print(tokenizer.decode(output[0], skip_special_tokens=False))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from textwrap import fill\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def run_chat(prompt,\n",
        "             system_prompt=\"You are a helpful assistant.\",\n",
        "             max_new_tokens=512,\n",
        "             temperature=0.3,\n",
        "             wrap_width=100,\n",
        "             return_text=False):\n",
        "    \"\"\"\n",
        "    Clean chat wrapper for LFM2 in Colab.\n",
        "    Shows only the assistant reply; no prompt echo, no duplicate output.\n",
        "    \"\"\"\n",
        "    # Build messages\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt.strip()},\n",
        "        {\"role\": \"user\",   \"content\": prompt.strip()},\n",
        "    ]\n",
        "\n",
        "    # Encode with chat template\n",
        "    input_ids = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        add_generation_prompt=True,\n",
        "        return_tensors=\"pt\",\n",
        "        tokenize=True,\n",
        "    ).to(model.device)\n",
        "\n",
        "    # Generate\n",
        "    output = model.generate(\n",
        "        input_ids,\n",
        "        do_sample=True,\n",
        "        temperature=temperature,\n",
        "        min_p=0.15,\n",
        "        repetition_penalty=1.05,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "    )\n",
        "\n",
        "    # ðŸ”‘ Decode only *new* tokens (no prompt echo)\n",
        "    gen_tokens = output[0, input_ids.shape[-1]:]\n",
        "    text = tokenizer.decode(gen_tokens, skip_special_tokens=True).strip()\n",
        "\n",
        "    # Render nicely (markdown if present, else wrapped plain text)\n",
        "    if any(x in text for x in (\"#\", \"* \", \"- \", \"`\", \"\\n\\n\")):\n",
        "        md = f\"**User:** {prompt}\\n\\n**Assistant:**\\n\\n{text}\"\n",
        "    else:\n",
        "        md = f\"**User:** {prompt}\\n\\n**Assistant:**\\n\\n{fill(text, width=wrap_width)}\"\n",
        "    display(Markdown(md))\n",
        "\n",
        "    # By default, do NOT return text to avoid Colab echo.\n",
        "    if return_text:\n",
        "        return text"
      ],
      "metadata": {
        "id": "dlwnRlqCKPg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example prompt\n",
        "run_chat(\"Explain edge computing and give one practical example.\")\n",
        "\n",
        "# You can reuse this cell with new prompts:\n",
        "# run_chat(\"What is the difference between AI and machine learning?\")\n",
        "# run_chat(\"Summarize the history of quantum computers.\")"
      ],
      "metadata": {
        "id": "Kj7kkCABIYoa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_chat(\"describe sarcobesity,  how to get rid off it\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Wb1qjj1eKmc8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZXz4dyrVNF6v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run_chat(\"How to address and reverse sarcopenia?\", max_new_tokens=1024)"
      ],
      "metadata": {
        "collapsed": true,
        "id": "nXK6M-EGLpiA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!jupyter nbconvert --ClearMetadataPreprocessor.enabled=True --inplace Copy_of_LFM2_MoE_Inference.ipynb"
      ],
      "metadata": {
        "id": "NHZfuh6BNK7h"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}